{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x136792790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 65 unique chars/tokens/iints\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# tokenize string to integer according to vocabulary, store in torch.tensor\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # mapping from unique chars to ints\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: stoi\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: itos\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:1000]) # first 1000 chars look like this to GPT\n",
    "# tensor is multi-dim mat/array [], dimensions such as batch size, sequence length (time dimension), embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train (90%) and validation (10%) sets\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs/contexts:\n",
      "torch.Size([4, 8])\n",
      "tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
      "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
      "        [43, 57, 58, 63,  6,  1, 58, 46]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47]])\n",
      "----\n",
      "when input is [57] the target: 43\n",
      "when input is [57, 43] the target: 60\n",
      "when input is [57, 43, 60] the target: 43\n",
      "when input is [57, 43, 60, 43] the target: 52\n",
      "when input is [57, 43, 60, 43, 52] the target: 1\n",
      "when input is [57, 43, 60, 43, 52, 1] the target: 63\n",
      "when input is [57, 43, 60, 43, 52, 1, 63] the target: 43\n",
      "when input is [57, 43, 60, 43, 52, 1, 63, 43] the target: 39\n",
      "when input is [60] the target: 43\n",
      "when input is [60, 43] the target: 42\n",
      "when input is [60, 43, 42] the target: 8\n",
      "when input is [60, 43, 42, 8] the target: 0\n",
      "when input is [60, 43, 42, 8, 0] the target: 25\n",
      "when input is [60, 43, 42, 8, 0, 25] the target: 63\n",
      "when input is [60, 43, 42, 8, 0, 25, 63] the target: 1\n",
      "when input is [60, 43, 42, 8, 0, 25, 63, 1] the target: 45\n",
      "when input is [56] the target: 42\n",
      "when input is [56, 42] the target: 5\n",
      "when input is [56, 42, 5] the target: 57\n",
      "when input is [56, 42, 5, 57] the target: 1\n",
      "when input is [56, 42, 5, 57, 1] the target: 57\n",
      "when input is [56, 42, 5, 57, 1, 57] the target: 39\n",
      "when input is [56, 42, 5, 57, 1, 57, 39] the target: 49\n",
      "when input is [56, 42, 5, 57, 1, 57, 39, 49] the target: 43\n",
      "when input is [43] the target: 57\n",
      "when input is [43, 57] the target: 58\n",
      "when input is [43, 57, 58] the target: 63\n",
      "when input is [43, 57, 58, 63] the target: 6\n",
      "when input is [43, 57, 58, 63, 6] the target: 1\n",
      "when input is [43, 57, 58, 63, 6, 1] the target: 58\n",
      "when input is [43, 57, 58, 63, 6, 1, 58] the target: 46\n",
      "when input is [43, 57, 58, 63, 6, 1, 58, 46] the target: 47\n"
     ]
    }
   ],
   "source": [
    "# plug in training data into transformer - train on random batches of chunks/blocks of train_data\n",
    "block_size = 8  # 8 characters, 8 contexts, 8 targets - time dimension\n",
    "# context (1): 18 | target: 47\n",
    "# context (2): 18, 47 | target: 56\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# context (block_size): 18, 47, 56, 57, 58, 1, 15, 47 | target: 58\n",
    "# transformer is used to seeing contexts from 1 to block_size\n",
    "batch_size = 4 # number of sequences/chunks\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate 4 random chunk positions\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # contexts\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # targets\n",
    "    return x, y # returns 4 batches of chunks of 8 tokens/chars for inputs and targets\n",
    "\n",
    "xb, yb = get_batch('train') # from train_data\n",
    "print('inputs/contexts:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "\n",
    "# xb input tensor is fed into transformer, transformer processes contexts, looks up correct target to predict in yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor([[ 0.3302,  1.4595, -1.7275,  ..., -1.4876, -0.7216, -1.6158],\n",
      "        [ 0.0537, -0.4753,  1.6139,  ..., -0.5359,  0.4087,  1.5254],\n",
      "        [-0.1348, -0.2903, -0.5741,  ...,  0.5060,  0.4991,  0.0977],\n",
      "        ...,\n",
      "        [-0.1262,  1.2708, -0.0055,  ..., -1.0670, -0.9107,  0.2090],\n",
      "        [ 0.5353,  0.7397, -1.3648,  ..., -0.6719,  0.9182,  1.0367],\n",
      "        [ 0.3575, -1.9182, -0.7526,  ...,  1.3856, -0.9983,  0.3111]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(4.5792, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "KDwe-zBAhLLcTEOSRS$jEKnYwfaD'-ErPM!nquxVkKeERb,Yo.p,zAdq'Ua$.mpdvt-cm :gH?VjIfV3KRnoZQRK,nADQAO3hE?y\n"
     ]
    }
   ],
   "source": [
    "# Feed into neural network: bigram language model\n",
    "# inputs (context indices/characters):\n",
    "# tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n",
    "#        [60, 43, 42,  8,  0, 25, 63,  1],\n",
    "#        [56, 42,  5, 57,  1, 57, 39, 49],\n",
    "#        [43, 57, 58, 63,  6,  1, 58, 46]])\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "# Bigram model predicts likelihood of a next token/word in a sequence based on previous word/token - depends only on the immediate previous one\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # ex: 57 will pluck out 57th row (65-dimensional embedding vector)\n",
    "        # each int token/char is represented by a 65-dimensional embedding vector [-0.2442, 0.1703, ...] where each channel/dimension of the vector represents the score for the next token - hence why we need 65 possible channels for 65 possible next tokens\n",
    "        # the embedding vector is not the semantic meaning of the char in this case, but is rather the scores/predictions of all possible next chars - these scores can be converted to a prob distr which is the predictions assigned to each label/class/token, the target is the ground truth label/class/token/index\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # 65x65 lookup table -- 65 unique tokens x 65 embedding channels/dimensions/next_token_scores\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) 4x8x65\n",
    "\n",
    "        # evaluate the loss between the predicted labels (probability distribution) and the target labels (ground truth)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # 32x65 stretch out to 2d\n",
    "            targets = targets.view(B*T) # 32 1d\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss # scores for next character in the sequence\n",
    "\n",
    "    # generate text up to max_new_tokens, continues generation of new tokens (8+1+2+3...max_new_tokens) in time dimension in all 4 batch dimensions:\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context (current batch of inputs)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx) # don't pass targets and no need to evaluate loss here in generate(), since loss is calculated in the forward function DURING TRAINING\n",
    "            # focus only on the last time step (last element in T dimension)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities (convert logits to prob distr)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # predicts one idx_next sample index for each T dimension (B, 1)\n",
    "            # append sampled index to the running sequence, whatever is predicted is concatenated on top of the previous idx along the time dimension\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel() # construct lm\n",
    "# Example random batches fed in:\n",
    "logits, loss = m(xb, yb) # pass in sample inputs (idx) and targets (to forward)\n",
    "print(logits.shape)\n",
    "print(logits)\n",
    "print(loss) # loss should be negative log likelihood: -ln(1/65) = 4.17\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # 0 (newline character) is how we kick of the generation B=1, T=1\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) # need to index into [0] row to pluck the single batch dimension (array of indices) generated\n",
    "\n",
    "# Ridiculous: right now we are feeding in the entire sequence, but since this is a bigram model, we only need the immediate previous token to make a prediction\n",
    "#   later, characters will look further back in the history sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of printing loss for each batch, estimate average loss\n",
    "eval_iters = 200\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss 2.4519, val loss 2.4920\n",
      "epoch 100: train loss 2.4479, val loss 2.4904\n",
      "epoch 200: train loss 2.4522, val loss 2.4941\n",
      "epoch 300: train loss 2.4538, val loss 2.4898\n",
      "epoch 400: train loss 2.4521, val loss 2.4790\n",
      "epoch 500: train loss 2.4521, val loss 2.4983\n",
      "epoch 600: train loss 2.4500, val loss 2.4832\n",
      "epoch 700: train loss 2.4572, val loss 2.4946\n",
      "epoch 800: train loss 2.4544, val loss 2.4939\n",
      "epoch 900: train loss 2.4604, val loss 2.4990\n",
      "epoch 1000: train loss 2.4448, val loss 2.4985\n",
      "epoch 1100: train loss 2.4538, val loss 2.4877\n",
      "epoch 1200: train loss 2.4526, val loss 2.4944\n",
      "epoch 1300: train loss 2.4561, val loss 2.4902\n",
      "epoch 1400: train loss 2.4503, val loss 2.4960\n",
      "epoch 1500: train loss 2.4478, val loss 2.4861\n",
      "epoch 1600: train loss 2.4486, val loss 2.4895\n",
      "epoch 1700: train loss 2.4541, val loss 2.4905\n",
      "epoch 1800: train loss 2.4532, val loss 2.4937\n",
      "epoch 1900: train loss 2.4557, val loss 2.4904\n",
      "epoch 2000: train loss 2.4478, val loss 2.4872\n",
      "epoch 2100: train loss 2.4553, val loss 2.4892\n",
      "epoch 2200: train loss 2.4456, val loss 2.4859\n",
      "epoch 2300: train loss 2.4553, val loss 2.4923\n",
      "epoch 2400: train loss 2.4542, val loss 2.5010\n",
      "epoch 2500: train loss 2.4513, val loss 2.4935\n",
      "epoch 2600: train loss 2.4597, val loss 2.4835\n",
      "epoch 2700: train loss 2.4498, val loss 2.4858\n",
      "epoch 2800: train loss 2.4529, val loss 2.4928\n",
      "epoch 2900: train loss 2.4636, val loss 2.4890\n",
      "epoch 3000: train loss 2.4527, val loss 2.4790\n",
      "epoch 3100: train loss 2.4485, val loss 2.4940\n",
      "epoch 3200: train loss 2.4574, val loss 2.4891\n",
      "epoch 3300: train loss 2.4626, val loss 2.4975\n",
      "epoch 3400: train loss 2.4531, val loss 2.4900\n",
      "epoch 3500: train loss 2.4593, val loss 2.4906\n",
      "epoch 3600: train loss 2.4562, val loss 2.4931\n",
      "epoch 3700: train loss 2.4493, val loss 2.5018\n",
      "epoch 3800: train loss 2.4559, val loss 2.4873\n",
      "epoch 3900: train loss 2.4352, val loss 2.4851\n",
      "epoch 4000: train loss 2.4549, val loss 2.4990\n",
      "epoch 4100: train loss 2.4590, val loss 2.4915\n",
      "epoch 4200: train loss 2.4466, val loss 2.4943\n",
      "epoch 4300: train loss 2.4497, val loss 2.4913\n",
      "epoch 4400: train loss 2.4555, val loss 2.4968\n",
      "epoch 4500: train loss 2.4562, val loss 2.4918\n",
      "epoch 4600: train loss 2.4471, val loss 2.4860\n",
      "epoch 4700: train loss 2.4502, val loss 2.4947\n",
      "epoch 4800: train loss 2.4475, val loss 2.4835\n",
      "epoch 4900: train loss 2.4455, val loss 2.4911\n",
      "\n",
      "Whin p BRI sus ngre k'tlld be; dedened andowee RYOUMy atheringn ngrstil blley, Istld hy you t gngrimosean t? acerder CEThe ais waverrkeveknthaul, 's w,'le r t m thicubina tcharar t ishofoverorcandet\n",
      "Yowonot.\n",
      "PShe. f h; veley, INENor Thisthary oul gou whur:\n",
      "Hawl mely hat, aitsthio lyby'stinithy s bend wintenoret,\n",
      "STEROulat saseplindeavirusifovixtly-\n",
      "\n",
      "F ghero m hin br; ofetowis.\n",
      "O, AUCllong as esioros.\n",
      "RKE:\n",
      "\n",
      "\n",
      "QUndouce,\n",
      "S:\n",
      "\n",
      "Fr:\n",
      "TENGoust qupage rey! th ty utoounoufll iss,\n",
      "Thyould ce Le d o thrtenne \n"
     ]
    }
   ],
   "source": [
    "### Train the model:\n",
    "# create a PyTorch optimizer - takes the gradients and updates the parameters (weights/channels/scores of the embedding vectors in the embedding table) using the gradients\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # stochastic gradient descent is simplest optimizer, but let's use AdamW, set learning rate to 1e-3\n",
    "batch_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "for iter in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"epoch {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb) # loss given the targets\n",
    "    optimizer.zero_grad(set_to_none=True) # clear previous iteration gradients\n",
    "    loss.backward() # loss is backpropagated to compute the gradients of the parameters w/respect to the loss\n",
    "    optimizer.step() # update the parameters\n",
    "\n",
    "# print(loss.item()) --each individual batch loss is noisy, so we need to average up losses using estimate_loss to get a better final value for loss\n",
    "\n",
    "### generate text from the model:\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # start from newline\n",
    "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "# Up to this point, we are using the simplest possible bigram model\n",
    "    # Tokens are not talking to each other, only looking at last char to predict\n",
    "    #Need a transformer so tokens can get better context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.2858,  0.9651],\n",
       "         [-2.0371,  0.4931],\n",
       "         [ 1.4870,  0.5910],\n",
       "         [ 0.1260, -1.5627],\n",
       "         [-1.1601, -0.3348],\n",
       "         [ 0.4478, -0.8016],\n",
       "         [ 1.5236,  2.5086]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 1.0101,  0.1215],\n",
       "         [ 0.1584,  1.1340],\n",
       "         [-1.1539, -0.2984],\n",
       "         [-0.5075, -0.9239],\n",
       "         [ 0.5467, -1.4948],\n",
       "         [-1.2057,  0.5718],\n",
       "         [-0.5974, -0.6937]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.3514, -0.2759],\n",
       "         [-1.5108,  2.1048],\n",
       "         [ 2.7630, -1.7465],\n",
       "         [ 1.4516, -1.5103],\n",
       "         [ 0.8212, -0.2115],\n",
       "         [ 0.7789,  1.5333],\n",
       "         [ 1.6097, -0.4032]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The mathematical trick in self-attention\n",
    "\n",
    "# toy example:\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels (information at each point in the sequence)\n",
    "x = torch.randn(B,T,C) # random tensor\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [ 0.1723, -0.0832],\n",
      "        [ 0.1737, -0.0819],\n",
      "        [ 0.1738, -0.0819],\n",
      "        [ 0.1737, -0.0818],\n",
      "        [ 0.1737, -0.0819],\n",
      "        [ 0.1737, -0.0819],\n",
      "        [ 0.1737, -0.0819]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [ 0.1765, -0.0766],\n",
      "        [ 0.1756, -0.0784],\n",
      "        [ 0.1751, -0.0792],\n",
      "        [ 0.1749, -0.0798],\n",
      "        [ 0.1747, -0.0801],\n",
      "        [ 0.1745, -0.0804],\n",
      "        [ 0.1744, -0.0805]])\n"
     ]
    }
   ],
   "source": [
    "# version 1:\n",
    "# 8 time dim sequence tokens are currently not talking to each other\n",
    "# element at 5th location should not talk to 6th, 7th, 8th location, only talk to 4th 3rd, 2nd, 1st\n",
    "#   can't talk to future\n",
    "# easiest way to communicate with past is to average all previous elements\n",
    "# take channels from all steps up to and including current location and average into a feature vector that summarizes current char in context of its history\n",
    "# Just doing an average has a lot of losses of context/space though, weak communication\n",
    "# For every batch element (Tth token in a sequence), calculate the average of all the vectors in the previous tokens and current token\n",
    "\n",
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bag of words\n",
    "for b in range(B): # iterate over batches, b is 0 to 3\n",
    "    for t in range(T): # iterate over sequence, t is 0 to 7\n",
    "        xprev = x[b,:t+1] # (t+1,C) 5x2\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # get a vector (1,C) like [mean0,mean1] that is stored in xbow[b,t] -- xbow will be (B,T,C)\n",
    "print(x[0])\n",
    "print(xbow[0]) # xbow[0][4] is average of x[0][0] to x[0][4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Above is inefficient\n",
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\" (feature vectors that summarize each char in context of its history)\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) #3x3 returns lower triangular portion of matrix\n",
    "a = a / torch.sum(a, 1, keepdim=True) # all rows of triangle sum to 1\n",
    "b = torch.randint(0,10,(3,2)).float() #3x2 random mat\n",
    "c = a @ b #3x2 matrix is averages\n",
    "\n",
    "# Mat mult:\n",
    "# a row 0 dot product b col 0 = c row 0 col 0\n",
    "# a row 0 dot product b col 1 = c row 0 col 1\n",
    "# a row 1 dot product b col 0 = c row 1 col 0\n",
    "# a row 1 dot product b col 1 = c row 1 col 1 ...\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# version 2:\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(\"weights:\")\n",
    "print(wei)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C) 8x8 @ 4x8x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3: use Softmax, we'll use this in self attention\n",
    "tril = torch.tril(torch.ones(T, T)) # tril is traingle of 1s TxT mat\n",
    "wei = torch.zeros((T,T)) # TxT mat of 0s, weights are interaction strengths/affinities/interest between tokens ( how much of each token do we want to average up/aggregate )\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # TxT mat of 0s bottom triangle, top triangle is -inf, tokens from the past can't communicate with the future\n",
    "wei = F.softmax(wei, dim=-1) # normalize\n",
    "# wei is 8 x 8 i.e. at 5th token the wei is [token_1_wei, token_2_wei, token_3_wei, token_4_wei, token_5_wei, 0, 0, 0], these values are how much attention token 5 is giving to itself and past tokens\n",
    "xbow3 = wei @ x # aggregate x values depending on tokens affinities/how interesting they find each other\n",
    "            # the values (x) will be the embedding vectors' channels (semantic meaning of each token)\n",
    "\n",
    "# i.e. token 5 has affinities to token 4, token 3, token 2, and token 1, particularly token 1.  Then token 5 pays more attention to token 1.  Self-attention adds more context to each token.\n",
    "# xbow3 is 4x8x2, where the embedding vectors channels C=2 are a context representation (weighted aggregation) for each token - capture semantic meaning of each character in the sequence\n",
    "# Can do weighted aggregation of past elements by using mat mult of a lower triangular fashion, and elements in the lower triangular part tell how much of each element fuses into this position/token\n",
    "\n",
    "# MATH BEHIND SELF-ATTENTION SUMMARY EXAMPLE:\n",
    "# wei=                                  affinities (T,T)\n",
    "# tensor([[1.0000, 0.0000, 0.0000],\t\ttoken 1\n",
    "#         [0.5000, 0.5000, 0.0000],\t\ttoken 2 pays 0.5000 of attention to itself, 0.5000 to token 1\n",
    "#         [0.3333, 0.3333, 0.3333]])\ttoken 3 pays 0.3333 attention to itself, 0.3333 to token 2, and 0.3333 to token 1\n",
    "\n",
    "# x=                                    values (B,T,C) - embedding vectors\n",
    "# tensor([[2., 7.],\t\t\t            token 1 semantic meaning\n",
    "#         [6., 4.],\t\t\t            token 2 semantic meaning\n",
    "#         [6., 5.]])\t\t\t        token 3 semantic meaning\n",
    "\n",
    "# xbow=wei @ x                          weighted aggregation (B,T,C) - contextual representation of each token\n",
    "# tensor([[2.0000, 7.0000],             token 1\n",
    "#         [4.0000, 5.5000],             token 2 semantic meaning given that it finds itself 0.5000 interesting, and token 2 0.5000 interesting (average of token 2 and token 1 semantic meanings)\n",
    "#         [4.6667, 5.3333]])\t        token 3 semantic meaning given that it pays 0.3333 attention to itself and 0.3333 to each past token (value depending on weight/affinity/interest/attention to itself and each other token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4407, -0.3253,  0.1413,  0.5404, -0.2668,  0.4908,  0.2691, -0.1132],\n",
      "        [-0.8334, -0.4139,  0.0260,  0.8446, -0.5456,  0.2604, -0.0139,  0.0732],\n",
      "        [-0.2557, -0.3152,  0.0191, -0.0953, -0.2461, -0.3576,  0.0187, -0.2387],\n",
      "        [ 0.1959, -0.2004, -0.0842, -0.2124, -0.1401, -0.2925, -0.3232, -0.2565],\n",
      "        [-0.3142,  0.0047, -0.1970, -0.3301,  0.5091,  0.2160,  0.0930,  0.2314],\n",
      "        [-0.0782,  0.6038, -0.0276, -0.2483,  0.8362, -0.6307,  0.3547,  0.3049],\n",
      "        [ 0.2719,  0.4913, -0.0655, -0.0789,  0.1523,  0.3154, -0.1371,  0.2012],\n",
      "        [-0.4511, -0.1031, -0.2077,  0.1475, -0.1997, -0.1464,  0.1608,  0.1576]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
      "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
      "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C) # tokens, each with identity and position as embedding vector of 32 channels\n",
    "\n",
    "# self-attention:\n",
    "# every single token at each position will emit 2 vectors: query, key\n",
    "# query vector is what am I looking for\n",
    "# key vector is what do I contain\n",
    "# the way we get affinities between tokens is dot product between keys and queries\n",
    "# my token query dot products with all the keys of all the other tokens and that dot product is wei\n",
    "# if key and query is aligned, high interaction\n",
    "\n",
    "# single head performing self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # project 32 channels of identity & position into 16 channels/dimensions (random)\n",
    "query = nn.Linear(C, head_size, bias=False) # project 32 channels of identity & position into 16 channels (random)\n",
    "value = nn.Linear(C, head_size, bias=False) # project 32 channels of identity & position into 16 channels (random)\n",
    "k = key(x) #(B, T, 16) (TRAITS I CONTAIN)\n",
    "q = query(x) #(B, T, 16) (TRAITS I AM LOOKING FOR)\n",
    "v = value(x) # x into v -- private information to this token, example: 5th token has some identity in x, I will communicate v, project x into v\n",
    "# 8th token has 16 query channels representing traits it wants, 4th token has 16 key channels representing its own traits, dot product of query and key leads to 8th token affinity value to 4th token \n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, 16) @ (B, 16, T) ----> (B, T, T) for every row of B, we now have a (T,T) matrix giving us the affinities\n",
    "print(wei[0])\n",
    "# data dependent wei\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # need to mask so no communication with future\n",
    "wei = F.softmax(wei, dim=-1) # normalize distribution to 1\n",
    "print(wei[0])\n",
    "out = wei @ v # (4,8,16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8th token knows its identity/content and its position - creates a query based on that looking for tokens with traits, all tokens emitted keys, one key channel could be traits and that key could have a high number in that channel, so when query and key dot product, they find each other and create a high affinity\n",
    "\n",
    "For example, let's say we have a token key vector that represents the token's traits. The key vector will have a high number in 4th channel (a vowel trait). If we have another token query vector that also has a high number in the 4th channel, the dot product between the two vectors will be higher, so there is a higher affinity\n",
    "\n",
    "UNDER THE HOOD: SINGLE HEAD OF SELF-ATTENTION MECHANISM EXAMPLE\n",
    "B,T,C=4,8,32\n",
    "head_size=16\n",
    "Let's look at batch 0, a sequence of 8 tokens.\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "q=                                                                                   queries (B,T,16) -- each query is traits I am looking for\n",
    "tensor([[-0.6567,  0.0283,  0.0094, -0.6995, -0.3604,  0.8376, -0.4446,  0.1228,\n",
    "          0.6276, -0.6222,  0.3483,  0.2411,  0.5409, -0.2605,  0.3612, -0.0436],\n",
    "        [-0.3932,  0.8220, -0.7027,  0.0954, -0.1222, -0.1518, -0.5024, -0.4636,\n",
    "          0.1176,  1.4282, -0.5812,  0.1401,  0.9604,  0.0410, -0.6214, -0.6347],\n",
    "        [ 0.2157, -0.3507,  0.0022,  0.4232, -0.2284, -0.0732, -0.3412,  0.9647,\n",
    "         -0.5178,  0.0921, -0.5043,  0.8388,  0.6149, -0.0109, -0.5569,  0.5820],\n",
    "        [ 0.9000, -0.1272,  0.5458,  0.4254, -0.4513, -0.0212,  0.1711,  0.2599,\n",
    "         -0.9978,  0.4890,  0.1737, -0.0700, -0.3113,  0.3748, -0.1848, -0.6379],\n",
    "        [ 0.0332,  0.5886, -0.4437,  0.3775, -0.6826, -0.2775,  0.4673, -1.2956,\n",
    "          0.6603,  0.1633, -1.7573, -0.6582, -0.2302, -0.0862, -0.0060,  0.7573],\n",
    "        [ 0.2098,  0.0439, -0.0702,  0.0727, -0.2012, -1.7539,  1.0369,  0.1163,\n",
    "          0.2956,  0.3231,  0.5052,  0.7011, -0.2844, -0.7844,  0.4782, -0.5170],\n",
    "        [ 0.6100, -0.3284, -0.8557,  0.8543,  0.7805, -0.4023, -0.8183, -0.0554,\n",
    "          0.1873,  0.2706, -0.7066, -0.8637,  0.6998, -0.0670,  0.2551,  0.2149],\n",
    "        [ 0.1459,  0.1349, -0.2335, -0.0417,  0.2928, -0.5080,  0.1177,  0.1861,\n",
    "          0.1455,  0.0292, -0.8470,  0.6116,  1.2445,  0.1909,  0.3694, -0.0027]],   8th token query- 8th token looking for 12th channel trait, high 13th\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "k=                                                                                   (B,T,16) keys -- each key is traits I contain\n",
    "tensor([[ 0.1196, -0.3013,  0.3629,  1.1771,  1.1385, -0.2554,  0.1454, -0.2944,\n",
    "         -0.7020, -1.0308,  0.7436, -0.8098, -0.6669,  0.0912, -0.0061,  0.1983],\n",
    "        [-0.5423, -0.5558, -0.0761,  1.2929,  0.8653, -1.1998,  0.3878,  0.1939,\n",
    "          0.7024, -0.8225,  0.2348, -0.8499, -0.3813, -0.2991,  0.0102, -0.5545],\n",
    "        [-0.3736, -0.4678, -0.2156, -0.8034, -0.3715, -0.5443, -0.9146, -0.0559,\n",
    "         -0.3290, -0.2102,  0.1166, -0.1798, -0.2820, -0.3320, -0.4596, -0.1325],\n",
    "        [-0.3146,  0.0845, -0.1235, -0.7058, -0.1802,  0.5492, -0.8980, -0.4938,\n",
    "          0.6791,  0.8827,  0.4911,  0.5190,  0.9011,  0.0913, -0.1933, -0.6770],     4th token key- high 12th channel trait, high 13th\n",
    "        [ 0.0239,  0.0998, -0.1871, -0.0860, -0.4881, -1.6765,  0.2413,  0.7361,\n",
    "          0.4608, -0.8722, -0.4259, -1.1347, -1.0571, -0.9401,  0.1343, -0.0157],\n",
    "        [-0.2362, -0.7873, -0.3802,  0.5815, -0.3722,  1.2405, -0.7004, -1.4917,\n",
    "          0.7678,  0.3584,  0.6120, -0.0794,  0.5983,  0.2635,  0.6490,  0.0709],\n",
    "        [-0.7941, -0.1660, -0.2810, -0.1021, -0.7352, -0.7518, -0.1276, -0.0051,\n",
    "          0.3325, -0.3374,  0.1678,  0.3105,  0.2258,  0.1243,  0.4617,  0.2016],\n",
    "        [ 0.1651, -0.1599, -0.5717, -0.3957,  0.3930, -0.8567,  0.3390, -0.7977,\n",
    "          0.2213, -0.5161,  0.1850, -0.2105,  0.3779,  0.0482, -0.4744, -0.0504]],\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "wei=q @ k.transpose(-2, -1) * head_size**-0.5=                                       affinities (B,T,T)- how much attention to myself and previous tokens\n",
    "8 queries dot products 8 keys gives 8 token affinities\n",
    "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
    "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
    "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
    "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
    "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
    "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
    "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
    "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],   8th token affinities -- 8th token query vector dot products 4th\n",
    "                                                                                        token key vector = 0.5898 so 8th token has high affinity with 4th token\n",
    "masking and softmax normalization\n",
    "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],          \n",
    "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
    "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
    "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],         8th token has 0.2391 affinity w itself, high 0.2297 affinity w 4th\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "v=                                                                                  values (B,T,16) - token identity & position embedding vectors\n",
    "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
    "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
    "        [ 0.8321, -0.8144, -0.3242,  0.5191, -0.1252, -0.4898, -0.5287, -0.0314,\n",
    "          0.1072,  0.8269,  0.8132, -0.0271,  0.4775,  0.4980, -0.1377,  1.4025],\n",
    "        [ 0.6035, -0.2500, -0.6159,  0.4068,  0.3328, -0.3910,  0.1312,  0.2172,\n",
    "         -0.1299, -0.8828,  0.1724,  0.4652, -0.4271, -0.0768, -0.2852,  1.3875],\n",
    "        [ 0.6657, -0.7096, -0.6099,  0.4348,  0.8975, -0.9298,  0.0683,  0.1863,\n",
    "          0.5400,  0.2427, -0.6923,  0.4977,  0.4850,  0.6608,  0.8767,  0.0746],\n",
    "        [ 0.1536,  1.0439,  0.8457,  0.2388,  0.3005,  1.0516,  0.7637,  0.4517,\n",
    "         -0.7426, -1.4395, -0.4941, -0.3709, -1.1819,  0.1000, -0.1806,  0.5129],\n",
    "        [-0.8920,  0.0578, -0.3350,  0.8477,  0.3876,  0.1664, -0.4587, -0.5974,\n",
    "          0.4961,  0.6548,  0.0548,  0.9468,  0.4511,  0.1200,  1.0573, -0.2257],\n",
    "        [-0.4849,  0.1655, -0.2221, -0.1345, -0.0864, -0.6628, -0.0936,  0.1050,\n",
    "         -0.2612,  0.1854,  0.3171, -0.1393,  0.5486, -0.4086, -0.3851,  0.7106],\n",
    "        [ 0.2042,  0.3772, -1.1255,  0.3995,  0.1489,  0.3590, -0.1791,  1.3732,\n",
    "          0.1588, -0.2320,  0.1651,  0.7604,  0.3521, -1.0864, -0.7939, -0.3025]],  8th token semantic meaning/identity & position\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "out = wei @ v                                                                       (B,T,16) weighted aggregation - contextual rep of each token\n",
    "semantic meaning & position of tokens given the weights (value depending on affinity with itself and each other token - attention)\n",
    "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
    "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
    "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
    "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
    "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
    "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
    "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
    "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
    "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
    "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
    "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
    "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
    "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
    "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
    "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
    "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],  8th token contextual representation -- meaning given communication w\n",
    "                                                                                        sequence tokens\n",
    "\n",
    "Notes:\n",
    "\n",
    "Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "\"self-attention\" just means that the keys and values are produced from the same source (x) as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module in encoder-decoder transformer) for context we'd like to condition on\n",
    "\"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much.\n",
    "Multi-head attention is multiple attentions in parallel and concatenating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "epoch 0: train loss 4.4399, val loss 4.4423\n",
      "epoch 100: train loss 2.6634, val loss 2.6812\n",
      "epoch 200: train loss 2.5023, val loss 2.4999\n",
      "epoch 300: train loss 2.3979, val loss 2.4116\n",
      "epoch 400: train loss 2.3234, val loss 2.3312\n",
      "epoch 500: train loss 2.2713, val loss 2.2845\n",
      "epoch 600: train loss 2.2138, val loss 2.2239\n",
      "epoch 700: train loss 2.1722, val loss 2.1902\n",
      "epoch 800: train loss 2.1308, val loss 2.1587\n",
      "epoch 900: train loss 2.0940, val loss 2.1309\n",
      "epoch 1000: train loss 2.0682, val loss 2.1032\n",
      "epoch 1100: train loss 2.0416, val loss 2.1022\n",
      "epoch 1200: train loss 2.0047, val loss 2.0534\n",
      "epoch 1300: train loss 1.9934, val loss 2.0460\n",
      "epoch 1400: train loss 1.9581, val loss 2.0193\n",
      "epoch 1500: train loss 1.9422, val loss 2.0139\n",
      "epoch 1600: train loss 1.9211, val loss 2.0184\n",
      "epoch 1700: train loss 1.9046, val loss 1.9889\n",
      "epoch 1800: train loss 1.8716, val loss 1.9805\n",
      "epoch 1900: train loss 1.8725, val loss 1.9623\n",
      "epoch 2000: train loss 1.8432, val loss 1.9675\n",
      "epoch 2100: train loss 1.8338, val loss 1.9622\n",
      "epoch 2200: train loss 1.8209, val loss 1.9411\n",
      "epoch 2300: train loss 1.8151, val loss 1.9255\n",
      "epoch 2400: train loss 1.8098, val loss 1.9192\n",
      "epoch 2500: train loss 1.7795, val loss 1.9138\n",
      "epoch 2600: train loss 1.7857, val loss 1.9147\n",
      "epoch 2700: train loss 1.7846, val loss 1.9168\n",
      "epoch 2800: train loss 1.7715, val loss 1.9041\n",
      "epoch 2900: train loss 1.7620, val loss 1.8976\n",
      "epoch 3000: train loss 1.7598, val loss 1.8890\n",
      "epoch 3100: train loss 1.7346, val loss 1.8913\n",
      "epoch 3200: train loss 1.7263, val loss 1.8800\n",
      "epoch 3300: train loss 1.7247, val loss 1.8778\n",
      "epoch 3400: train loss 1.7201, val loss 1.8648\n",
      "epoch 3500: train loss 1.7121, val loss 1.8650\n",
      "epoch 3600: train loss 1.7004, val loss 1.8736\n",
      "epoch 3700: train loss 1.6993, val loss 1.8560\n",
      "epoch 3800: train loss 1.6912, val loss 1.8695\n",
      "epoch 3900: train loss 1.6882, val loss 1.8458\n",
      "epoch 4000: train loss 1.6760, val loss 1.8403\n",
      "epoch 4100: train loss 1.6855, val loss 1.8524\n",
      "epoch 4200: train loss 1.6685, val loss 1.8371\n",
      "epoch 4300: train loss 1.6751, val loss 1.8243\n",
      "epoch 4400: train loss 1.6735, val loss 1.8279\n",
      "epoch 4500: train loss 1.6645, val loss 1.8313\n",
      "epoch 4600: train loss 1.6591, val loss 1.8137\n",
      "epoch 4700: train loss 1.6517, val loss 1.8159\n",
      "epoch 4800: train loss 1.6429, val loss 1.8202\n",
      "epoch 4900: train loss 1.6446, val loss 1.8165\n",
      "\n",
      "And you, I, they for the deaven.\n",
      "\n",
      "For Gttlarenges blood where's amant,\n",
      "That ous short of Done, and the made know? it,\n",
      "Or the bock byself withouts ass boy?\n",
      "\n",
      "Prits say sboul,\n",
      "Provant suke goart, thy hotempt, I am\n",
      "shold of highave\n",
      "Tharrelitatent. This the eyest:\n",
      "The wilt, stay thy graventood, but,\n",
      "Whereign at was sword I'll him: not prove\n",
      "Of you cress bark'd eat st: you must;\n",
      "Bresshem time comend on they.\n",
      "\n",
      "ISABELLA:\n",
      "And you did it: I met say broth afd I have I love again,\n",
      "Hast love acraship fiently\n"
     ]
    }
   ],
   "source": [
    "# Final Transformer Script\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "batch_size = 16 # number of sequences/chunks B\n",
    "block_size = 32  # 32 characters, 32 contexts, 32 targets - time dimension T\n",
    "n_embd = 64     # C\n",
    "learning_rate = 1e-3\n",
    "n_head = 4 # head_size will be n_embd // 4 = 16 dimensional\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# read in\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 65 unique chars/tokens/iints\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# tokenize string to integer according to vocabulary, store in torch.tensor\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # mapping from unique chars to ints\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: stoi\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: itos\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# tensor is multi-dim mat/array [], dimensions such as batch size, sequence length (time dimension), embedding size\n",
    "\n",
    "# split data into train (90%) and validation (10%) sets\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# plug in training data into transformer - train on random batches of chunks/blocks of train_data\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate 4 random chunk positions\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # contexts\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # targets\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y # returns 4 batches of chunks of 8 tokens/chars for inputs and targets\n",
    "\n",
    "# xb input tensor is fed into transformer, transformer processes contexts, looks up correct target to predict in yb\n",
    "\n",
    "class Head(nn.Module):\n",
    "    ''' one head of self-attention '''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # project n_embd channels of identity & position into head_size channels/dimensions (random)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) \n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape   # C is n_embd\n",
    "        k = self.key(x)   # (B,T,head_size)\n",
    "        q = self.query(x) # (B,T,head_size)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,head_size)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concatenates back to n_embd C\n",
    "        out = self.dropout(self.proj(out)) # back into residual pathway\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # projection layer going back into the residual pathway\n",
    "            nn.Dropout(dropout), # prevent overfitting\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    # constructor\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, h_head: number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head # 4 16-dimensional self-attention heads\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # sa and concatenates back to n_embd C\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # residual connections - fork off and come back:\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Feed into neural network: bigram language model\n",
    "# inputs (context indices/characters):\n",
    "# tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n",
    "#        [60, 43, 42,  8,  0, 25, 63,  1],\n",
    "#        [56, 42,  5, 57,  1, 57, 39, 49],\n",
    "#        [43, 57, 58, 63,  6,  1, 58, 46]])\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "# Bigram model predicts likelihood of a next token/word in a sequence based on previous word/token - depends only on the immediate previous one\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # ex: 57 will pluck out 57th row (65-dimensional embedding vector)\n",
    "        # each int token/char is represented by a 65-dimensional embedding vector [-0.2442, 0.1703, ...] where each channel/dimension of the vector represents the score for the next token - hence why we need 65 possible channels for 65 possible next tokens\n",
    "        # the embedding vector is not the semantic meaning of the char in this case, but is rather the scores/predictions/logits of all possible next chars - these scores can be converted to a prob distr which is the predictions assigned to each label/class/token, the target is the ground truth label/class/token/index\n",
    "        # self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # 65x65 lookup table -- 65 unique tokens x 65 embedding channels/dimensions/next_token_scores\n",
    "    \n",
    "        # SELF-ATTENTION VERSION:\n",
    "        # for self-attention implementation, need a level of indirection, the embedding vector is not directly the scores/logits of next chars, is the semantic meaning\n",
    "        # n_embd is number of embedding dimensions\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #65xn_embd lookup table, will be updated during training\n",
    "        # to go from token embeddings to logits, need a linear layer\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # converts 16x32x64 token embeddings to 16x32x65 logits\n",
    "        # don't encode just the identities of idx tokens, but also position\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # 32x64 position table, updated during training\n",
    "        #self.sa_head = Head(n_embd)\n",
    "        #self.sa_heads = MultiHeadAttention(4, n_embd//4) # i.e. 4 16-dimensional self-attention heads concatenates back to n_embd C\n",
    "        #self.ffwd = FeedForward(n_embd)\n",
    "        #self.blocks = nn.Sequential(\n",
    "        #    Block(n_embd, n_head=4),\n",
    "        #    Block(n_embd, n_head=4),\n",
    "        #    Block(n_embd, n_head=4),\n",
    "        #    nn.LayerNorm(n_embd),\n",
    "        #)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "\n",
    "        #logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        # SELF-ATTENTION VERSION:\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd C) 16x32x64 no longer logits, but token embeddings - each token has embedding vector of n_embd channels\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C) x holds token identities and positions where they occur\n",
    "        #x = self.sa_heads(x) # apply heads of self-attention (B,T,head_size)\n",
    "        #x = self.ffwd(x) # (B,T,C) layer for thinking on self-attended data\n",
    "        x = self.blocks(x) # (B,T,n_embd C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size) converts the token embedding vectors and positions to scores/logits\n",
    "\n",
    "        # evaluate the loss between the predicted labels (probability distribution) and the target labels (ground truth)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss # scores for next character in the sequence\n",
    "\n",
    "    # generate text up to max_new_tokens, continues generation of new tokens (8+1+2+3...max_new_tokens) in time dimension in all 4 batch dimensions:\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context (current batch of inputs)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) # don't pass targets and no need to evaluate loss here in generate(), since loss is calculated in the forward function DURING TRAINING\n",
    "            # focus only on the last time step (last element in T dimension)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities (convert logits to prob distr)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # predicts one idx_next sample index for each T dimension (B, 1)\n",
    "            # append sampled index to the running sequence, whatever is predicted is concatenated on top of the previous idx along the time dimension\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        \n",
    "model = BigramLanguageModel() # construct lm\n",
    "# Ridiculous old version: right now we are feeding in the entire sequence, but since this is a bigram model, we only need the immediate previous token to make a prediction\n",
    "#   later, characters will look further back in the history sequence: self attention\n",
    "\n",
    "# Instead of printing loss for each batch, estimate average loss\n",
    "eval_iters = 200\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "### Train the model:\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "# create a PyTorch optimizer - takes the gradients and updates the parameters (weights/channels/scores of the embedding vectors in the embedding table) using the gradients\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # stochastic gradient descent is simplest optimizer, but let's use AdamW, set learning rate to 1e-3\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "for iter in range(max_iters): # increase number of steps for good results... \n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"epoch {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # loss given the targets\n",
    "    optimizer.zero_grad(set_to_none=True) # clear previous iteration gradients\n",
    "    loss.backward() # loss is backpropagated to compute the gradients of the parameters w/respect to the loss\n",
    "    optimizer.step() # update the parameters\n",
    "\n",
    "# print(loss.item()) --each individual batch loss is noisy, so we need to average up losses using estimate_loss to get a better final value for loss\n",
    "\n",
    "### generate text from the model:\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # 0 (newline character) is how we kick of the generation B=1, T=1\n",
    "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist())) # need to index into [0] row to pluck the single batch dimension (array of indices) generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side Notes:\n",
    "\n",
    "The nn.Linear module in PyTorch takes a tensor of input features and applies a linear transformation to it, resulting in a tensor of output features. The linear transformation is a matrix multiplication, where the input features are multiplied by a weight matrix and then added to a bias term.\n",
    "\n",
    "In the case of your token embeddings, the input features are the 32-dimensional embedding vector for each token. The weight matrix has 32 rows (one for each embedding dimension) and 65 columns (one for each possible token). The bias term has 65 elements.\n",
    "\n",
    "The linear transformation is then applied to each of the 8 token embeddings, resulting in a tensor of 8 65-dimensional vectors. These vectors represent the scores for the next token in the sequence, for each of the 8 possible tokens.\n",
    "\n",
    "The output of the nn.Linear module is a tensor of shape (4, 8, 65), which is the same shape as the input token embeddings, but with 65 output features instead of 32."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18c54ba209731a9a645a57e9b9447f0dc298123110b3f3158c178dc85ee6ba84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
